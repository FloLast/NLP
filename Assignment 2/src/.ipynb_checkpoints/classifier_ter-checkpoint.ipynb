{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize        \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB # With alpha=0.53 acc=78.72\n",
    "from sklearn.svm import LinearSVC # With C=0.138 acc=79.26\n",
    "\n",
    "path_train = '/Users/Flore/python/NLP/Assignment 2/data/traindata.csv'\n",
    "path_test = '/Users/Flore/python/NLP/Assignment 2/data/devdata.csv'\n",
    "\n",
    "class Classifier:\n",
    "    \"\"\"The Classifier\"\"\"\n",
    "\n",
    "    # A useful function to clean and lemmatize the data\n",
    "    def create_sentence(dataset):\n",
    "        '''As input : the column to be lemmatized.\n",
    "        This function gives as output a list of strings, \n",
    "        corresponding to the lemmatized words.'''\n",
    "        clean_data = []\n",
    "        for row in dataset:\n",
    "            sentence = ''\n",
    "            for word in row:\n",
    "                sentence += word + ' '\n",
    "            clean_data.append(sentence)\n",
    "        return clean_data\n",
    "\n",
    "    def train(self, trainfile):\n",
    "        \"\"\"Trains the classifier model on the training set stored in file trainfile\"\"\"\n",
    "\n",
    "        # We load the data and lower the text\n",
    "        data_train = pd.read_csv(trainfile, sep = \"\\t\", names = [\"polarity\", \"category\", \"word\", \"offsets\", \"sentence\"])\n",
    "        data_train['sentence_l'] = data_train['sentence'].apply(str.lower)\n",
    "        data_train['word'] = data_train['word'].apply(str.lower)\n",
    "        \n",
    "        # We try to keep all the no/nor/not words as this changes radically the sentiment analysis\n",
    "        data_train['sentence_l'] = data_train[\"sentence_l\"].apply(lambda sentence: sentence.replace(\"can\\'t\", \"can not\"))\n",
    "        data_train['sentence_l'] = data_train[\"sentence_l\"].apply(lambda sentence: sentence.replace(\"n\\'t\", \" not\"))\n",
    "        self.stopwords = stopwords.words(\"english\")\n",
    "        self.stopwords.remove('nor')\n",
    "        self.stopwords.remove('no')\n",
    "        self.stopwords.remove('not')\n",
    "        \n",
    "        # We clean the train data and stem the words\n",
    "        self.stemmer = nltk.porter.PorterStemmer()\n",
    "        clean_sentences = []\n",
    "        for row in data_train['sentence_l']:\n",
    "            tokens = word_tokenize(row)\n",
    "            tokens = [word for word in tokens if word.isalpha()]\n",
    "            tokens = [w for w in tokens if not w in self.stopwords] \n",
    "            tokens = [self.stemmer.stem(word) for word in tokens]\n",
    "            clean_sentences.append(tokens)\n",
    "        data_train['stems'] = clean_sentences\n",
    "        \n",
    "        # We also stem the target words to be coherent with the stemmed words in the sentences\n",
    "        data_train['word'] = [self.stemmer.stem(word) for word in data_train['word']]\n",
    "    \n",
    "        # We recreate the sentences with the selected and cleaned words\n",
    "        Classifier.create_sentence = staticmethod(Classifier.create_sentence)\n",
    "        data_train.clean_sentence = Classifier.create_sentence(data_train.stems)\n",
    "        \n",
    "        # We create a BOW vector\n",
    "        self.restaurant_vect = CountVectorizer(min_df=1, tokenizer=nltk.word_tokenize)\n",
    "        reviews_counts = self.restaurant_vect.fit_transform(data_train.clean_sentence)\n",
    "    \n",
    "        # We transform the BOW vector with the tfidf scores\n",
    "        self.tfidf_transformer = TfidfTransformer()\n",
    "        reviews_tfidf = self.tfidf_transformer.fit_transform(reviews_counts)\n",
    "        \n",
    "        polarities = []\n",
    "        for row in data_train['polarity']:\n",
    "            if row == 'positive':\n",
    "                polarities.append(1)\n",
    "            if row == 'neutral':\n",
    "                polarities.append(0)\n",
    "            if row == 'negative':\n",
    "                polarities.append(-1)\n",
    "        data_train['polarity_floats'] = polarities\n",
    "        \n",
    "        # Split data into training and test sets\n",
    "        test_size = 10\n",
    "        X_train, X_test, y_train, y_test = train_test_split(reviews_tfidf, data_train.polarity_floats,\n",
    "                                                            test_size = test_size/100, random_state = None)\n",
    "        \n",
    "        ############# CNN MODEL ##############\n",
    "        \n",
    "        # MLP for the IMDB problem\n",
    "        import numpy\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense\n",
    "        from keras.layers import Flatten\n",
    "        from keras.layers.embeddings import Embedding\n",
    "        # fix random seed for reproducibility\n",
    "        seed = 7\n",
    "        numpy.random.seed(seed)\n",
    "        \n",
    "        top_words = 5000\n",
    "        sequence_length = X_train.shape[1]\n",
    "        \n",
    "        # create the model\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(top_words, 32, input_length=sequence_length))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(250, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "        # Final evaluation of the model\n",
    "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "        \n",
    "\n",
    "    def predict(self, datafile):\n",
    "        \"\"\"Predicts class labels for the input instances in file 'datafile'\n",
    "        Returns the list of predicted labels\n",
    "        \"\"\"\n",
    " \n",
    "        # We load the test data and lower the text\n",
    "        data_test = pd.read_csv(datafile, sep = \"\\t\", names = [\"polarity\", \"category\", \"word\", \"offsets\", \"sentence\"])\n",
    "        data_test['sentence_l'] = data_test['sentence'].apply(str.lower)\n",
    "        data_test['word'] = data_test['word'].apply(str.lower)\n",
    "        \n",
    "        # We try to keep all the no/nor/not words as this changes radically the sentiment analysis\n",
    "        data_test['sentence_l'] = data_test[\"sentence_l\"].apply(lambda sentence: sentence.replace(\"can\\'t\", \"can not\"))\n",
    "        data_test['sentence_l'] = data_test[\"sentence_l\"].apply(lambda sentence: sentence.replace(\"n\\'t\", \" not\"))\n",
    "        \n",
    "        # We clean the data and stem the words\n",
    "        clean_sentences = []\n",
    "        for row in data_test['sentence_l']:\n",
    "            tokens = word_tokenize(row)\n",
    "            tokens = [word for word in tokens if word.isalpha()]\n",
    "            tokens = [w for w in tokens if not w in self.stopwords] \n",
    "            tokens = [self.stemmer.stem(word) for word in tokens]\n",
    "            clean_sentences.append(tokens)\n",
    "        data_test['stems'] = clean_sentences\n",
    "        \n",
    "        # We also stem the target words to be coherent with the stemmed words in the sentences\n",
    "        data_test['word'] = [self.stemmer.stem(word) for word in data_test['word']]\n",
    "\n",
    "        # We recreate the sentences with the selected and cleaned words\n",
    "        Classifier.create_sentence = staticmethod(Classifier.create_sentence)\n",
    "        data_test.clean_sentence = Classifier.create_sentence(data_test.stems)\n",
    "        \n",
    "        # We create a BOW vector\n",
    "        reviews_new_counts = self.restaurant_vect.transform(data_test.clean_sentence)\n",
    "        \n",
    "        # We transform the BOW vector with the tfidf scores\n",
    "        reviews_new_tfidf = self.tfidf_transformer.transform(reviews_new_counts)\n",
    "        \n",
    "        # We make a prediction with the classifier\n",
    "        self.pred = self.model.predict(reviews_new_tfidf)\n",
    "        \n",
    "        return self.pred\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
